{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niv0902/Shablool/blob/main/SearchEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez3Zp6NhFGqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58445fb-5159-4c7c-d684-f8facd8ce1a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "adding stop words\n",
        "1.general stopwords\n",
        "2.stop words that are relevent to the subject -"
      ],
      "metadata": {
        "id": "GBnF--YbUwXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to remove stop wards from dictionary- (current text)"
      ],
      "metadata": {
        "id": "zJGVNVS6U9gT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.Create the Hash Table that counts the amount of stem words from the text\n",
        "2.Stem the words"
      ],
      "metadata": {
        "id": "YCrGxoaeVSAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "def apply_stemming(index):\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_index = {}\n",
        "  for word, count in index.items():\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    if stemmed_word in stemmed_index:\n",
        "      stemmed_index[stemmed_word] += count\n",
        "    else:\n",
        "      stemmed_index[stemmed_word] = count\n",
        "  return stemmed_index\n"
      ],
      "metadata": {
        "id": "X1id2HTFVkDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function\n",
        "1.change all words to lower case\n",
        "2. use stemming- it allows to search for sentences and not only single words"
      ],
      "metadata": {
        "id": "q8EOHCWIWVet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. index_words - all words from the url\n",
        "2. remove stop words - remove the stop words\n",
        "3. apply stemming - change the words to the stem form\n",
        "4. search - finds the words that are relevent and return thier location in the text"
      ],
      "metadata": {
        "id": "hOAQx479x_BC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "class searchEngine:\n",
        "  def __init__(self):\n",
        "    \"\"\"Initialize the search engine\"\"\"\n",
        "    self.base_url = \"https://mqtt.org\"\n",
        "    self.pages = []\n",
        "    self.word_locations = defaultdict(list)  # word -> [(page_id, frequency), ...]\n",
        "    self.stop_words = {\n",
        "        'a', 'an', 'the', 'and', 'or', 'but', 'if', 'then', 'else', 'for', 'on', 'in', 'with',\n",
        "        'is', 'are', 'was', 'were', 'be', 'being', 'been',\n",
        "        'of', 'to', 'from', 'by', 'as', 'at', 'it', 'its', 'that', 'this', 'these', 'those',\n",
        "        'we', 'you', 'he', 'she', 'they', 'i', 'me', 'my', 'your', 'our', 'their',\n",
        "        'which', 'who', 'whom', 'what', 'when', 'where', 'why', 'how', 'can', 'will', 'should', 'could'\n",
        "    }\n",
        "    self.stemmer = PorterStemmer()\n",
        "  def fetch_pages(self, urls):\n",
        "    \"\"\"Fetch pages from specific MQTT.org URLs\"\"\"\n",
        "    try:\n",
        "        for url in urls:\n",
        "            full_url = f\"{self.base_url}{url}\"\n",
        "            print(f\"Fetching: {full_url}\")\n",
        "            response = requests.get(full_url)\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Failed to fetch {full_url}\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            title = soup.title.get_text(strip=True) if soup.title else 'No Title'\n",
        "            content = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "            page_id = len(self.pages) + 1\n",
        "            self.pages.append({\n",
        "                'id': page_id,\n",
        "                'title': title,\n",
        "                'url': full_url,\n",
        "                'content': content\n",
        "            })\n",
        "        print(f\"Retrieved {len(self.pages)} pages.\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def build_index(self):\n",
        "    \"\"\"Build a simple word location index\"\"\"\n",
        "    self.word_locations.clear()\n",
        "\n",
        "    # Process each page\n",
        "    for page in self.pages:\n",
        "        # Get all words from content\n",
        "        words = re.findall(r'\\w+', page['content'].lower())\n",
        "\n",
        "        # Count word frequencies\n",
        "        word_counts = defaultdict(int)\n",
        "        for word in words:\n",
        "            if word not in self.stop_words:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        # Stemming\n",
        "        word_counts = self.apply_stemming(word_counts)\n",
        "\n",
        "        # Add to index with page information\n",
        "        for word, count in word_counts.items():\n",
        "            self.word_locations[word].append((page['id'], count))\n",
        "\n",
        "  def apply_stemming(self, word_counts):\n",
        "    \"\"\"Apply stemming to the word counts\"\"\"\n",
        "    return {self.stemmer.stem(word): count for word, count in word_counts.items()}\n",
        "\n",
        "  def search(self, query, num_results=5):\n",
        "    \"\"\"Search pages using simple word frequency ranking\"\"\"\n",
        "    # Get query words and apply stemming\n",
        "    query_words = [\n",
        "        self.stemmer.stem(word.lower())\n",
        "        for word in re.findall(r'\\w+', query)\n",
        "        if word.lower() not in self.stop_words\n",
        "    ]\n",
        "\n",
        "    if not query_words:\n",
        "        return []\n",
        "\n",
        "    # Calculate scores for each page\n",
        "    page_scores = defaultdict(lambda: {'matches': 0, 'total_freq': 0})\n",
        "\n",
        "    for word in query_words:\n",
        "        for page_id, freq in self.word_locations.get(word, []):\n",
        "            page_scores[page_id]['matches'] += 1\n",
        "            page_scores[page_id]['total_freq'] += freq\n",
        "\n",
        "    # Sort by the number of matches, then by total frequency\n",
        "    ranked_results = [\n",
        "        (page_id, scores['matches'], scores['total_freq'])\n",
        "        for page_id, scores in page_scores.items()\n",
        "    ]\n",
        "    ranked_results.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
        "\n",
        "    # Format results\n",
        "    results = []\n",
        "    for page_id, matches, total_freq in ranked_results[:num_results]:\n",
        "        page = next(p for p in self.pages if p['id'] == page_id)\n",
        "        original_query_words = [\n",
        "            word.lower()\n",
        "            for word in re.findall(r'\\w+', query)\n",
        "            if word.lower() not in self.stop_words\n",
        "        ]\n",
        "        context = self.get_context(page['content'], original_query_words)\n",
        "        results.append({\n",
        "            'title': page['title'],\n",
        "            'url': page['url'],\n",
        "            'matching_words': matches,\n",
        "            'total_frequency': total_freq,\n",
        "            'context': context\n",
        "        })\n",
        "    return results\n",
        "\n",
        "  def get_context(self, content, query_words, window=30):\n",
        "    \"\"\"Get a short snippet of text around the first matching word\"\"\"\n",
        "    content_lower = content.lower()\n",
        "    for word in query_words:\n",
        "        index = content_lower.find(word)\n",
        "        if index != -1:\n",
        "            start = max(index - window, 0)\n",
        "            end = min(index + window, len(content))\n",
        "            return content[start:end].strip()\n",
        "    return \"No context found.\"\n"
      ],
      "metadata": {
        "id": "Q6aZA1cqcOst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engine = searchEngine()\n",
        "\n",
        "urls = [\n",
        "    '/getting-started/',\n",
        "    '/mqtt-specification/',\n",
        "    '/use-cases/',\n",
        "    '/software/',\n",
        "    '/faq/',\n",
        "    ''\n",
        "]\n",
        "\n",
        "engine.fetch_pages(urls)\n",
        "engine.build_index()\n",
        "\n",
        "results = engine.search(\"publish subscribe model\")\n",
        "\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"Result {i}:\")\n",
        "    print(f\"Title: {result['title']}\")\n",
        "    print(f\"URL: {result['url']}\")\n",
        "    print(f\"Matching Words: {result['matching_words']}\")\n",
        "    print(f\"Total Frequency: {result['total_frequency']}\")\n",
        "    print(f\"Context: {result['context']}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY5ka_GufoCV",
        "outputId": "68ad8460-9b4f-4d19-9008-9fa12efe4095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching: https://mqtt.org/getting-started/\n",
            "Fetching: https://mqtt.org/mqtt-specification/\n",
            "Fetching: https://mqtt.org/use-cases/\n",
            "Fetching: https://mqtt.org/software/\n",
            "Fetching: https://mqtt.org/faq/\n",
            "Fetching: https://mqtt.org\n",
            "Retrieved 6 pages.\n",
            "Result 1:\n",
            "Title: Software\n",
            "URL: https://mqtt.org/software/\n",
            "Matching Words: 3\n",
            "Total Frequency: 3\n",
            "Context: TT clients while allowing the publishers to reliably transmi\n",
            "------------------------------------------------------------\n",
            "Result 2:\n",
            "Title: MQTT - The Standard for IoT Messaging\n",
            "URL: https://mqtt.org\n",
            "Matching Words: 2\n",
            "Total Frequency: 4\n",
            "Context: d as an extremely lightweight publish/subscribe messaging tr\n",
            "------------------------------------------------------------\n",
            "Result 3:\n",
            "Title: MQTT Specification\n",
            "URL: https://mqtt.org/mqtt-specification/\n",
            "Matching Words: 2\n",
            "Total Frequency: 2\n",
            "Context: such as Zigbee. MQTT-SN is a publish/subscribe messaging pr\n",
            "------------------------------------------------------------\n",
            "Result 4:\n",
            "Title: FAQ\n",
            "URL: https://mqtt.org/faq/\n",
            "Matching Words: 2\n",
            "Total Frequency: 2\n",
            "Context: for IoT connectivity. It is a publish/subscribe, extremely s\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yD7q6i8KrOxn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}